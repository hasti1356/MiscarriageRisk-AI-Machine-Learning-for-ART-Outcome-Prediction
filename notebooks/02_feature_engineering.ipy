{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Performance Analysis\n",
    "\n",
    "This notebook evaluates the trained model's performance and analyzes its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "from src.model import MiscarriageRiskModel\n",
    "from src.utils import MetricsCalculator\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data and predictions\n",
    "test_data = pd.read_csv('../data/processed/test_data.csv')\n",
    "predictions = np.load('../results/test_predictions.npy')\n",
    "\n",
    "X_test = test_data.drop('miscarriage_risk', axis=1)\n",
    "y_test = test_data['miscarriage_risk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate metrics\n",
    "calculator = MetricsCalculator()\n",
    "metrics = calculator.calculate_metrics(y_test, predictions > 0.5, predictions)\n",
    "\n",
    "# Display metrics\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "plot_confusion_matrix(y_test, predictions > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_roc_pr_curves(y_true, y_prob):\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    \n",
    "    # Calculate Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax1.plot(fpr, tpr)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curve')\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    ax2.plot(recall, precision)\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_roc_pr_curves(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot prediction distributions for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=pd.DataFrame({'predictions': predictions, 'actual': y_test}),\n",
    "             x='predictions', hue='actual', bins=30)\n",
    "plt.title('Distribution of Predictions by Actual Class')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze misclassified cases\n",
    "misclassified = X_test.copy()\n",
    "misclassified['actual'] = y_test\n",
    "misclassified['predicted'] = predictions > 0.5\n",
    "misclassified['prediction_prob'] = predictions\n",
    "\n",
    "errors = misclassified[misclassified['actual'] != misclassified['predicted']]\n",
    "\n",
    "print(\"Summary of Misclassified Cases:\")\n",
    "print(errors.describe())\n",
    "\n",
    "# Analyze feature distributions for misclassified cases\n",
    "for feature in X_test.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=misclassified, x='actual', y=feature, hue='predicted')\n",
    "    plt.title(f'{feature} Distribution for Correct vs Incorrect Predictions')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Plot calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, predictions, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Observed Proportion')\n",
    "plt.title('Calibration Plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}

